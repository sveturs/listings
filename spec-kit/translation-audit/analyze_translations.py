#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –¥–ª—è Sve Tu –ø—Ä–æ–µ–∫—Ç–∞
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å, –Ω–∞—Ö–æ–¥–∏—Ç –æ—à–∏–±–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import defaultdict

class TranslationAnalyzer:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.messages_dir = self.project_root / 'frontend/svetu/src/messages'
        self.src_dir = self.project_root / 'frontend/svetu/src'
        
        self.languages = ['en', 'ru', 'sr']
        self.report = {
            'summary': {},
            'critical_errors': [],
            'missing_translations': [],
            'incorrect_paths': [],
            'duplicate_keys': [],
            'unused_keys': [],
            'structure_issues': [],
            'recommendations': []
        }
        
    def analyze(self) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
        print("üîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤...")
        
        # 1. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤
        self._analyze_file_structure()
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–π –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏
        self._check_key_consistency()
        
        # 3. –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –∫–æ–¥–µ
        self._analyze_code_usage()
        
        # 4. –ü–æ–∏—Å–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
        self._check_structure_issues()
        
        # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        self._generate_recommendations()
        
        return self.report
        
    def _analyze_file_structure(self):
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
        print("üìÅ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–æ–≤...")
        
        files_by_lang = {}
        for lang in self.languages:
            lang_dir = self.messages_dir / lang
            if not lang_dir.exists():
                self.report['critical_errors'].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —è–∑—ã–∫–∞: {lang}")
                continue
                
            files = [f.stem for f in lang_dir.glob('*.json') if f.stem != 'index']
            files_by_lang[lang] = set(files)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–¥–∏–Ω–∞–∫–æ–≤–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏
        all_modules = set()
        for files in files_by_lang.values():
            all_modules.update(files)
            
        for lang in self.languages:
            lang_files = files_by_lang.get(lang, set())
            missing = all_modules - lang_files
            extra = lang_files - all_modules
            
            if missing:
                for module in missing:
                    self.report['missing_translations'].append({
                        'type': 'missing_module_file',
                        'language': lang,
                        'module': module,
                        'severity': 'critical'
                    })
                    
            if extra:
                for module in extra:
                    self.report['structure_issues'].append({
                        'type': 'extra_module_file',
                        'language': lang,
                        'module': module,
                        'severity': 'warning'
                    })
        
        self.report['summary']['total_modules'] = len(all_modules)
        self.report['summary']['languages'] = len(self.languages)
        
    def _check_key_consistency(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–π –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏"""
        print("üîë –ü—Ä–æ–≤–µ—Ä—è—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π...")
        
        modules_data = defaultdict(dict)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥—É–ª–∏
        for lang in self.languages:
            lang_dir = self.messages_dir / lang
            if not lang_dir.exists():
                continue
                
            for json_file in lang_dir.glob('*.json'):
                if json_file.stem == 'index':
                    continue
                    
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        modules_data[json_file.stem][lang] = data
                except json.JSONDecodeError as e:
                    self.report['critical_errors'].append({
                        'type': 'json_parse_error',
                        'file': str(json_file),
                        'error': str(e)
                    })
                except Exception as e:
                    self.report['critical_errors'].append({
                        'type': 'file_read_error',
                        'file': str(json_file),
                        'error': str(e)
                    })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–π
        total_missing = 0
        for module_name, lang_data in modules_data.items():
            base_lang = 'en'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ –±–∞–∑–æ–≤—ã–π
            if base_lang not in lang_data:
                continue
                
            base_keys = self._get_all_keys(lang_data[base_lang])
            
            for lang in self.languages:
                if lang == base_lang or lang not in lang_data:
                    continue
                    
                lang_keys = self._get_all_keys(lang_data[lang])
                
                missing = base_keys - lang_keys
                extra = lang_keys - base_keys
                
                for key in missing:
                    self.report['missing_translations'].append({
                        'type': 'missing_key',
                        'module': module_name,
                        'language': lang,
                        'key': key,
                        'severity': 'high'
                    })
                    total_missing += 1
                    
                for key in extra:
                    self.report['structure_issues'].append({
                        'type': 'extra_key',
                        'module': module_name,
                        'language': lang,
                        'key': key,
                        'severity': 'low'
                    })
        
        self.report['summary']['missing_keys_total'] = total_missing
        
    def _get_all_keys(self, data: Dict, prefix: str = '') -> Set[str]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–∏ –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞"""
        keys = set()
        
        for key, value in data.items():
            current_key = f"{prefix}.{key}" if prefix else key
            keys.add(current_key)
            
            if isinstance(value, dict):
                keys.update(self._get_all_keys(value, current_key))
                
        return keys
        
    def _analyze_code_usage(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –∫–æ–¥–µ"""
        print("üíª –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –∫–æ–¥–µ...")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        use_translations_pattern = re.compile(r"useTranslations\(['\"]([^'\"]+)['\"]\)")
        translation_call_pattern = re.compile(r"t\(['\"]([^'\"]+)['\"]")
        
        used_modules = set()
        used_keys = set()
        incorrect_paths = []
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º TypeScript/JavaScript —Ñ–∞–π–ª–∞–º
        for file_path in self.src_dir.rglob('*.{tsx,ts,jsx,js}'):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # –ò—â–µ–º useTranslations
                    for match in use_translations_pattern.finditer(content):
                        module_path = match.group(1)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –ø—É—Ç–∏ (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ)
                        if '.' in module_path:
                            incorrect_paths.append({
                                'file': str(file_path.relative_to(self.project_root)),
                                'line': content[:match.start()].count('\n') + 1,
                                'incorrect_path': module_path,
                                'suggested_fix': f"useTranslations('{module_path.split('.')[0]}')"
                            })
                        else:
                            used_modules.add(module_path)
                    
                    # –ò—â–µ–º –≤—ã–∑–æ–≤—ã t()
                    for match in translation_call_pattern.finditer(content):
                        key = match.group(1)
                        used_keys.add(key)
                        
                except Exception as e:
                    self.report['structure_issues'].append({
                        'type': 'file_analysis_error',
                        'file': str(file_path),
                        'error': str(e)
                    })
        
        self.report['incorrect_paths'] = incorrect_paths
        self.report['summary']['used_modules'] = len(used_modules)
        self.report['summary']['used_keys'] = len(used_keys)
        self.report['summary']['incorrect_paths'] = len(incorrect_paths)
        
    def _check_structure_issues(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ JSON —Ñ–∞–π–ª–∞—Ö"""
        print("üèóÔ∏è –ü—Ä–æ–≤–µ—Ä—è—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã...")
        
        structure_issues = 0
        
        for lang in self.languages:
            lang_dir = self.messages_dir / lang
            if not lang_dir.exists():
                continue
                
            for json_file in lang_dir.glob('*.json'):
                if json_file.stem == 'index':
                    continue
                    
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    issues = self._validate_json_structure(data, json_file.stem, lang)
                    structure_issues += len(issues)
                    self.report['structure_issues'].extend(issues)
                    
                except Exception:
                    # –û—à–∏–±–∫–∏ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ _check_key_consistency
                    pass
                    
        self.report['summary']['structure_issues'] = structure_issues
        
    def _validate_json_structure(self, data: Dict, module: str, lang: str) -> List[Dict]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON"""
        issues = []
        
        def check_recursive(obj, path=""):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    
                    if isinstance(value, dict):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã
                        if not value:
                            issues.append({
                                'type': 'empty_object',
                                'module': module,
                                'language': lang,
                                'path': current_path
                            })
                        else:
                            check_recursive(value, current_path)
                    elif isinstance(value, str):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        if not value.strip():
                            issues.append({
                                'type': 'empty_string',
                                'module': module,
                                'language': lang,
                                'path': current_path
                            })
                    
        check_recursive(data)
        return issues
        
    def _generate_recommendations(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        print("üí° –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
        
        recommendations = []
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
        if self.report['critical_errors']:
            recommendations.append({
                'priority': 'critical',
                'category': '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏',
                'description': f"–ù–∞–π–¥–µ–Ω–æ {len(self.report['critical_errors'])} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–æ–∫–∏—Ä—É—é—Ç —Ä–∞–±–æ—Ç—É —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤",
                'action': "–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"
            })
        
        # –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã
        if self.report['missing_translations']:
            high_priority = [t for t in self.report['missing_translations'] if t.get('severity') == 'high']
            recommendations.append({
                'priority': 'high',
                'category': '–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã',
                'description': f"–ù–∞–π–¥–µ–Ω–æ {len(high_priority)} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞",
                'action': "–î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã –≤–æ –≤—Å–µ —è–∑—ã–∫–æ–≤—ã–µ —Ñ–∞–π–ª—ã"
            })
        
        # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏
        if self.report['incorrect_paths']:
            recommendations.append({
                'priority': 'medium',
                'category': '–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤',
                'description': f"–ù–∞–π–¥–µ–Ω–æ {len(self.report['incorrect_paths'])} —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø—É—Ç–µ–π –≤ useTranslations",
                'action': "–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫—Ä–∏–ø—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—Ä—É—á–Ω—É—é"
            })
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        if self.report['structure_issues']:
            recommendations.append({
                'priority': 'low',
                'category': '–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã',
                'description': f"–ù–∞–π–¥–µ–Ω–æ {len(self.report['structure_issues'])} —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º",
                'action': "–û—á–∏—Å—Ç–∏—Ç—å –ø—É—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤"
            })
        
        self.report['recommendations'] = recommendations

def main():
    project_root = '/data/hostel-booking-system'
    analyzer = TranslationAnalyzer(project_root)
    report = analyzer.analyze()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    output_file = Path(project_root) / 'spec-kit/translation-audit/reports/analysis_results.json'
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:")
    print(f"   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏: {len(report['critical_errors'])}")
    print(f"   - –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã: {len(report['missing_translations'])}")
    print(f"   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—É—Ç–∏: {len(report['incorrect_paths'])}")
    print(f"   - –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: {len(report['structure_issues'])}")

if __name__ == '__main__':
    main()